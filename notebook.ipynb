{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c67f6c-39d6-41bd-83c0-c3f45dbfda04",
   "metadata": {},
   "source": [
    "# Keyword Classification on Google Speech Commands Dataset\n",
    "\n",
    "This project explores and processes the [Google Speech Commands Dataset](https://arxiv.org/abs/1804.03209) to build and train a model for recognizing speech commands. The notebook provides step-by-step data preprocessing, feature extraction, model definition, and training.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "- **`GSpeechComd_Dataset_with_Comments.ipynb`**: The main Jupyter Notebook file that includes code, detailed comments, and explanations for processing the dataset and training a model. Different CNN architectures are trained and evaluated.\n",
    "-[Google Speech Commands Dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html): The Google Speech Commands dataset, which contains short audio clips of spoken words, used as the primary data source.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Preprocessing**: Loading and preparing the dataset for training.\n",
    "- **Feature Extraction**: Applying signal processing techniques to extract features from the audio data.\n",
    "- **Model Training**: Using a machine learning framework (e.g., TensorFlow, PyTorch) to train a speech recognition model.\n",
    "- **Model Evaluation**: Accuracy calculations for the models performance.\n",
    "\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "This project was completed with assistance from various sources:\n",
    "1. The [Google Speech Commands Dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) for the dataset and initial problem inspiration.\n",
    "2. Online resources and tutorials related to audio processing and deep learning. Particularly: [Valerio Velardo's Audio Signal Processing for Machine Learning Course](https://youtube.com/playlist?list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&si=AwcQMXpYCuYQE9wV)\n",
    "3. The `ChatGPT` AI assistant by OpenAI for guidance and feedback in enhancing the notebook and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df910b-8d6a-4762-873b-7096fdb6797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torchaudio.transforms import Resample, MelSpectrogram, AmplitudeToDB, PitchShift\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf18c9-1429-4712-b845-e53e28693215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple functions for playing audio and showing spectrograms\n",
    "play = lambda x : ipd.display(ipd.Audio(x,rate=16000))\n",
    "def show(spec, title='Spectrogram'):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.imshow(spec, aspect='auto', origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6f5a0-7a7e-47ae-96c7-088705167706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data set and choosing the labels to classfy. More/less classes could be choosen, you only need to change num_class variable\n",
    "data_path = \"Google Speech Comands/\"\n",
    "classes_set = {\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"}\n",
    "classes = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c5163-e625-4be4-b02b-c97e1834795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    return torch.tensor(classes.index(word))\n",
    "def index_to_label(index):\n",
    "    return classes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09e2e0-2fe4-41c2-b816-3083dcdaa833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A subset initilization for not classifying the whole labels\n",
    "class SubsetSpeechCommands(SPEECHCOMMANDS):\n",
    "    def __init__(self, *args, transform = None, augment = False, classes, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.classes = classes\n",
    "        self.filtered_paths = [path for path in self._walker if self._load_class(path) in self.classes]\n",
    "        \n",
    "    def __getitem__(self, n):\n",
    "        path = self.filtered_paths[n]\n",
    "        label = label_to_index(self._load_class(path))\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        if self.augment:\n",
    "            # Frequency Shifting\n",
    "            n_steps = random.randint(-4, 4)\n",
    "            waveform = torch.Tensor(librosa.effects.pitch_shift(waveform.numpy(), sr=sample_rate, n_steps=n_steps))\n",
    "            # Adding noise    \n",
    "            noise = torch.randn_like(waveform)\n",
    "            waveform = waveform + 0.005 * noise\n",
    "        else:\n",
    "            pass\n",
    "        spec = self.transform(waveform)\n",
    "        return spec, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filtered_paths)\n",
    "\n",
    "    def _load_class(self, path):\n",
    "        return path.split('\\\\')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6c330-a9d3-44bf-98c1-74d0a0d815bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_pad(x,length):\n",
    "    pad_len = length - x.size()[2]\n",
    "    return F.pad(x,(0,pad_len),value=-80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e129eb82-2ff4-4e7a-acf5-cb0a7cc189eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmnorm(spectrogram):\n",
    "    min_val = torch.min(spectrogram)\n",
    "    max_val = torch.max(spectrogram)\n",
    "    normalized_spec = (spectrogram - min_val) / (max_val - min_val)\n",
    "    return normalized_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d5b14-b4b5-4ba0-a47b-32029bb3c18e",
   "metadata": {},
   "source": [
    "***Model and Learning***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcc3a6-9e6a-43e8-bb49-d47f0257c2f1",
   "metadata": {},
   "source": [
    "***Model - CNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67b673-1847-429b-a49b-8af426aec0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWithDropout(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNWithDropout, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layers with Dropout\n",
    "        self.fc1 = nn.Linear(128 * 16 * 10, 512)  # Adjust the input size based on the output of the last pooling layer\n",
    "        self.drop1 = nn.Dropout(p=0.5)  # Dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model_with_dropout = CNNWithDropout(num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a4fdf-48f3-4d0c-bd01-49bf3b70116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform and the dataloader\n",
    "\n",
    "res = torchaudio.transforms.Resample(orig_freq=16000, new_freq=8000)\n",
    "mel = torchaudio.transforms.MelSpectrogram(sample_rate=8000, n_mels=128, n_fft=400, hop_length =100)\n",
    "db = torchaudio.transforms.AmplitudeToDB(top_db=70)\n",
    "transform = lambda x : spec_pad(mmnorm(db(mel(res(x)))),81)\n",
    "\n",
    "\n",
    "def create_dataloader(train_size=3000, batch_size=32,augmented = False):\n",
    "    train_dataset = SubsetSpeechCommands(data_path,download=False, subset='training', transform=transform, classes=classes_set)\n",
    "    lengtht = len(train_dataset)\n",
    "    random.seed(42)\n",
    "    random_samples = random.sample(range(0, lengtht), train_size)\n",
    "    subset = Subset(train_dataset, random_samples)\n",
    "\n",
    "    #Applying data augmentation if the augmented parameter is True:\n",
    "    if augmented:\n",
    "        train_dataset_aug = SubsetSpeechCommands(data_path,download=False, subset='training', transform=transform, classes=classes_set, augment = True)\n",
    "        random_samples = random.sample(range(0, lengtht), 3000)\n",
    "        subset_aug = Subset(train_dataset_aug, random_samples)\n",
    "        subset = ConcatDataset([subset,subset_aug])\n",
    "    else:\n",
    "        pass\n",
    "    train_dataloader = DataLoader(subset, batch_size=32, shuffle=True)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9df50a-b79b-4e25-bda6-aab48d493ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create valid_dataset\n",
    "valid_dataset = SubsetSpeechCommands(data_path,download=False, subset='validation', transform=transform, classes=classes_set)\n",
    "lengthv = len(valid_dataset)\n",
    "random.seed(42)\n",
    "random_samples = random.sample(range(0, lengthv), 3000)\n",
    "subset = Subset(valid_dataset, random_samples)\n",
    "valid_dataloader = DataLoader(subset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b28b9-a374-4b91-97fa-c02c6838d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    num_epochs = 50\n",
    "    patience = 5  # Number of epochs to wait before stopping if no improvement\n",
    "    min_delta = 0.0001  # Minimum change in the monitored metric to qualify as an improvement\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    best_model_wts = None  # To store the best model weights\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "        \n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_dataloader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(valid_dataloader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping check and model saving\n",
    "        if val_loss < best_loss - min_delta:\n",
    "            best_loss = val_loss\n",
    "            counter = 0  # Reset counter if there's an improvement\n",
    "            best_model_wts = model.state_dict()  # Save the best model weights\n",
    "            print(\"Best model saved\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # After training is complete, load the best model weights\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(\"Loaded the best model weights from epoch with lowest validation loss.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faebc66-fe70-4f40-b669-0abdeabac8b7",
   "metadata": {},
   "source": [
    "Model Evaluations - result below, also you can run the codes in your local  \n",
    "First I trained with a smaller train subset to experiment with parameters. train_size parameter defines the size of the subset of train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37892de0-3d57-4a56-b89e-aeea10d26c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_dropout = CNNWithDropout(num_classes=20)\n",
    "train_dataloader = create_dataloader(train_size=3000, batch_size=64,augmented = False)\n",
    "model_with_dropout = train(model_with_dropout)\n",
    "eval(model_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b969e4-1a0f-421c-b0ef-160ca3472471",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_dropout = CNNWithDropout(num_classes=20)\n",
    "train_dataloader = create_dataloader(train_size=6000, batch_size=64,augmented = False)\n",
    "model_with_dropout = train(model_with_dropout)\n",
    "eval(model_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806a904-aafe-4d7e-9e87-ea07693e431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_dropout = CNNWithDropout(num_classes=20)\n",
    "train_dataloader = create_dataloader(train_size=3000, batch_size=64,augmented = True)\n",
    "model_with_dropout = train(model_with_dropout)\n",
    "eval(model_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed791eb-8643-49fb-a778-c8ddc53715d2",
   "metadata": {},
   "source": [
    "Final training with whole train (with augmentation) set with choosen classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faeef93-27a3-4b88-a1f2-cd803b244de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SubsetSpeechCommands(data_path,download=False, subset='training', transform=transform, classes=classes_set)\n",
    "train_dataset_aug = SubsetSpeechCommands(data_path,download=False, subset='training', transform=transform, classes=classes_set, augment = True)\n",
    "dataset = ConcatDataset([train_dataset,train_dataset_aug])\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635564ea-c0a9-4ef7-82ab-06a8931d16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_dropout = CNNWithDropout(num_classes=20)\n",
    "model_with_dropout = train(model_with_dropout)\n",
    "eval(model_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac8fe8-4f4e-405c-a20e-e027b5e8a226",
   "metadata": {},
   "source": [
    "Saving the models parameters, you can download the parameters from the github, no need to train it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6da42-df80-4221-9db6-5f17a6de52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNWithDropout(num_classes=20)\n",
    "model.load_state_dict(torch.load('mode_0.1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b514cbf-57bc-42c5-b77c-6b0b819e9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SubsetSpeechCommands(data_path,download=False, subset='test', transform=transform, classes=classes_set)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d2a89-cace-489f-a5b2-3270980981da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_dataloader:\n",
    "            outputs = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7d6a1-6f33-4b4d-b9c8-6516dfe80406",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e34e5-2cfe-4703-9fe9-3c26476fbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can examine the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm = pd.DataFrame(cm, labels=classes, columns= classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed175d-baaa-4dea-a929-07b21af02286",
   "metadata": {},
   "source": [
    "My results:\n",
    "# Model Accuracy Table\n",
    "\n",
    "| Model and dataset                                  | Train Size             | Accuracy  |\n",
    "|--------------------------------------------------  |------------------------|-----------|\n",
    "| Basic CNN, No augmentation                         | 3000                   | 0.609583  |\n",
    "| Basic CNN, Dropout layers added                    | 3000                   | 0.625595  |\n",
    "| Dropout layers added, train size increased         | 12000                  | 0.797213  |\n",
    "| Same model as above, Data augmented (3000 -> 12000)| 12000                  | 0.749541  |\n",
    "| Same model as above, whole  train dataset                                      | Full (~120000)         | 0.914313  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f08732-a6f8-4e2e-983b-f050bcd28b98",
   "metadata": {},
   "source": [
    "Old Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085881d-a956-47c6-ad32-ad82b6340fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN without Dropout layers. Dropout layers prevent overfitting. Adding dropout layers increase the performance.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 16 * 10, 512)  # Adjust the input size based on the output of the last pooling layer\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "# Instantiate the model\n",
    "model = CNN(num_classes=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1277d18-02ec-4929-ad24-4a8bb80d7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN with batch normalization. However, it did not increase the accuracy. In the end batch normalization is not used.\n",
    "class CNNWithBatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNWithBatchNorm, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with Batch Normalization\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 16 * 10, 512)  # Adjust the input size based on the output of the last pooling layer\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model_with_bn = CNNWithBatchNorm(num_classes=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
